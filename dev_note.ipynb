{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320123\n",
      "118\n",
      "\t\n",
      " !\"#%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_abcdefghijklmnopqrstuvwxyz{|}~²¾×á˙αγθϵ–—‘’“”•…−√∧∨≈△▽\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "chars = sorted(list(set(text)))\n",
    "print(len(chars))\n",
    "print(''.join(chars))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 68, 88, 88, 13, 2, 71, 68, 81, 68, 8, 82, 2, 82, 78, 76, 68, 2, 83, 68, 82, 83, 2, 83, 68, 87, 83, 13, 2, 65, 64, 65, 88]\n",
      "heyy, here's some test text, baby\n"
     ]
    }
   ],
   "source": [
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for i, s in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "test = \"heyy, here's some test text, baby\"\n",
    "print(encode(test))\n",
    "print(decode(encode(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([320123]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text))\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting btw training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([46, 14, 21,  1,  1, 34, 65, 82, 83])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demonstrating variable context between 1 and block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([46]):14\n",
      "tensor([46, 14]):21\n",
      "tensor([46, 14, 21]):1\n",
      "tensor([46, 14, 21,  1]):1\n",
      "tensor([46, 14, 21,  1,  1]):34\n",
      "tensor([46, 14, 21,  1,  1, 34]):65\n",
      "tensor([46, 14, 21,  1,  1, 34, 65]):82\n",
      "tensor([46, 14, 21,  1,  1, 34, 65, 82]):83\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "Y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = Y[t]\n",
    "    print(f'{context}:{target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([32, 8])\n",
      "tensor([[ 83,  82,   2,  83,  71,  64,  83,   1],\n",
      "        [ 78,  84,  81,  77,  64,  75,   2,  78],\n",
      "        [ 82,   2,  78,  77,   2,  65,  68,  83],\n",
      "        [ 14,  58,   2,  15,   1,  60,  19,  62],\n",
      "        [ 78,  67,   2,  79,  81,  68,  67,  72],\n",
      "        [ 77,  82,   2,  69,  78,  81,   2,  48],\n",
      "        [ 81,  72,  78,   2,  64,  82,   2,  83],\n",
      "        [  2,  82,  79,  64,  66,  68,  15,   1],\n",
      "        [ 42,   2,  66,  78,  84,  75,  67,   2],\n",
      "        [ 72,  83,  71,   2,  37,  53,  36,   1],\n",
      "        [ 82,   2,  77,  78,  83,   2,  64,  77],\n",
      "        [ 15,  78,  81,  70,  13,   2,  19,  17],\n",
      "        [ 68,  67,   2,  72,  77,  83,  78,   2],\n",
      "        [ 84,  72,  66,  74,   2,  86,  72,  83],\n",
      "        [  2,  83,  71,  68,   2,  83,  86,  78],\n",
      "        [  2,  83,  71,  68,   2,  77,  68,  87],\n",
      "        [  2,  51,  64,  85,  68,  77, 106,  82],\n",
      "        [ 67,   2,  82,  78,   2,  69,  64,  81],\n",
      "        [  2,  86,  72,  83,  71,   2,  77,  68],\n",
      "        [ 71,  64,  83,   2,  83,  71,  68,   2],\n",
      "        [ 78,   2,  86,  64,  88,   2,  83,  78],\n",
      "        [ 67,   2,  85,  68,  81,  65,  64,  75],\n",
      "        [ 15,  21,   2,  56,  71,  64,  83,   2],\n",
      "        [ 84,  83,   1,  66,  75,  64,  82,  82],\n",
      "        [ 13,   2,  65,  78,  83,  71,   2,  78],\n",
      "        [ 51,  48,  55,  38,  46,  38,  47,  53],\n",
      "        [  2,  46,  64,  66,  71,  72,  77,  68],\n",
      "        [ 72,  77,  68,   2,  45,  68,  64,  81],\n",
      "        [  2,  36,  78,  75,  78,  76,  65,  72],\n",
      "        [ 68,   2,  83,  64,  87,  72,   2,  72],\n",
      "        [ 71,   2,  83,  71,  68,   2,  66,  75],\n",
      "        [ 81,  84,  70,  70,  75,  68,  82,   1]])\n",
      "targets: torch.Size([32, 8])\n",
      "tensor([[ 82,   2,  83,  71,  64,  83,   1,  83],\n",
      "        [ 84,  81,  77,  64,  75,   2,  78,  69],\n",
      "        [  2,  78,  77,   2,  65,  68,  83,  83],\n",
      "        [ 58,   2,  15,   1,  60,  19,  62,   2],\n",
      "        [ 67,   2,  79,  81,  68,  67,  72,  66],\n",
      "        [ 82,   2,  69,  78,  81,   2,  48,  65],\n",
      "        [ 72,  78,   2,  64,  82,   2,  83,  71],\n",
      "        [ 82,  79,  64,  66,  68,  15,   1,  19],\n",
      "        [  2,  66,  78,  84,  75,  67,   2,  71],\n",
      "        [ 83,  71,   2,  37,  53,  36,   1,  64],\n",
      "        [  2,  77,  78,  83,   2,  64,  77,   1],\n",
      "        [ 78,  81,  70,  13,   2,  19,  17,  18],\n",
      "        [ 67,   2,  72,  77,  83,  78,   2,  64],\n",
      "        [ 72,  66,  74,   2,  86,  72,  83,  71],\n",
      "        [ 83,  71,  68,   2,  83,  86,  78,   2],\n",
      "        [ 83,  71,  68,   2,  77,  68,  87,  83],\n",
      "        [ 51,  64,  85,  68,  77, 106,  82,   2],\n",
      "        [  2,  82,  78,   2,  69,  64,  81,  27],\n",
      "        [ 86,  72,  83,  71,   2,  77,  68,  70],\n",
      "        [ 64,  83,   2,  83,  71,  68,   2,  52],\n",
      "        [  2,  86,  64,  88,   2,  83,  78,   2],\n",
      "        [  2,  85,  68,  81,  65,  64,  75,  75],\n",
      "        [ 21,   2,  56,  71,  64,  83,   2,  64],\n",
      "        [ 83,   1,  66,  75,  64,  82,  82,  72],\n",
      "        [  2,  65,  78,  83,  71,   2,  78,  69],\n",
      "        [ 48,  55,  38,  46,  38,  47,  53,   1],\n",
      "        [ 46,  64,  66,  71,  72,  77,  68,   1],\n",
      "        [ 77,  68,   2,  45,  68,  64,  81,  77],\n",
      "        [ 36,  78,  75,  78,  76,  65,  72,  64],\n",
      "        [  2,  83,  64,  87,  72,   2,  72,  77],\n",
      "        [  2,  83,  71,  68,   2,  66,  75,  72],\n",
      "        [ 84,  70,  70,  75,  68,  82,   1,  42]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split:str):\n",
    "    dat = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(dat) - block_size, (batch_size,))\n",
    "    x = torch.stack([dat[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([dat[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(f'inputs: {xb.shape}')\n",
    "print(xb)\n",
    "print(f'targets: {yb.shape}')\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bigram language model (simplest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 118]) tensor(5.0969, grad_fn=<NllLossBackward0>)\n",
      "\t0•kjyd&fH¾fB(.~—AtF0*1<×t\tB=’[1KK]FJpk=0∧/kgAX•|√*\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    '''\n",
    "    bigram only looks at the previous character in predicting the next\n",
    "    '''\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        #\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is not None:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T,)\n",
    "            # cross_entropy expects channels as second dim\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else: \n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tok):\n",
    "        # idx is (B,T) array if indicies in current context\n",
    "        for _ in range(max_new_tok):\n",
    "            logits, loss = self(idx) # get predictions\n",
    "            logits = logits[:, -1, :] # look at last timestep\n",
    "            probs = F.softmax(logits, dim=-1) # get probabilities from softmax\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # sample from prob dist\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # append sample\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size=vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape, loss)\n",
    "\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long) # 1x1 tensor of newline\n",
    "print(decode(m.generate(idx, max_new_tok=50)[0].tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyTorch optimizer\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6595726013183594\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tB:/6% melype C spe ales o a stitustithisteripuras. 16 ndrass n ovesong be ass arorthexios in MSingumicore n f t woons we f aivevios me Itnd ndongnffte stheleliewex pff tondde hefincer tatornin a as ge, cinous .\n",
      "US. aripserg t d PTikithaghasse fre \n",
      "d? 9028. a o gll p sserer s rry llapr bellig afoncarfe 1. a, o teinveand ch mbehemithaturar 0 me tioflisingucowile mpapttiodendi\n",
      "SA-\n",
      "[30.  s resse s 7 ctanthond s \n",
      "iches timase en tandilureareroryingall cecongrk-pre MI  qum can led.5\n",
      "eywhe OBiopodat an\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long) # 1x1 tensor of newline\n",
    "print(decode(m.generate(idx, max_new_tok=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a generative model, attention at a given character should only be applied to its predecessors``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted aggregation of predecessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C)) # x bag of words (average)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more efficient with matrix multiplication and lower triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / torch.sum(wei, 1, keepdim=True)\n",
    "xbow2 = wei @ x # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "torch.allclose(xbow,xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "masking 0 to -inf and using softmax. Why? This allows us to have the weights learned (rathe than identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T)) # affinity weights (dot prod of query and keys)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # <- diff btw encoder and decoder blocks\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention is a ***communication*** mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B,T,16)\n",
    "q = query(x) # (B,T,16)\n",
    "wei = q @ k.transpose(-2,-1) * head_size**-0.5 # (B,T,16) @ (B,16,T) ---> (B,T,T)\n",
    "# 1/sqrt(head_size) normalizes the variance, which affects softmax \"sharpness\"\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "with open('training_data_cleaned.txt','r') as f_c:\n",
    "    data_str = f_c.read()\n",
    "data_shards = data_str.split('\\n\\n')\n",
    "print(len(data_shards))\n",
    "random.seed(42)\n",
    "random.shuffle(data_shards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "here text is bro-dogg test :)\n"
     ]
    }
   ],
   "source": [
    "a = 'test text is here bro-dogg :)'\n",
    "a_shards = a.split(' ')\n",
    "print(len(a_shards))\n",
    "random.seed(42)\n",
    "random.shuffle(a_shards)\n",
    "new_a = ' '.join(a_shards)\n",
    "print(new_a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
